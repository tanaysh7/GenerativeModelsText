{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23FovOPuGf7j"
   },
   "source": [
    "# Generative Models for Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDeHnOZ_Gf7l"
   },
   "source": [
    "### (a) In this problem, we are trying to build a generative model to mimic the writing style of prominent British Mathematician, Philosopher, proliﬁc writer, and political activist, Bertrand Russell. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 7295,
     "status": "ok",
     "timestamp": 1525406809402,
     "user": {
      "displayName": "Tanay Shankar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "101648044313053317537"
     },
     "user_tz": 420
    },
    "id": "HvAerJk2Gf7n",
    "outputId": "484df3fb-e7df-44d2-cf5d-235889e0750d"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback,ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import one_hot\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io,os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJ8dMekPGf7z"
   },
   "source": [
    "### (b) Download the following books from Project Gutenberg http://www.gutenberg. org/ebooks/author/355 in text format: \n",
    "\n",
    "i. The Problems of Philosophy \n",
    "\n",
    "ii. The Analysis of Mind \n",
    "\n",
    "iii. Mysticism and Logic and Other Essays \n",
    "\n",
    "iv. Our Knowledge of the External World as a Field for Scientiﬁc Method in Philosophy Project Gutenberg adds a standard header and footer to each book and this is not part of the original text. Open the ﬁle in a text editor and delete the header and footer. The header is obvious and ends with the text: ** START OF THIS PROJECT GUTENBERG EBOOK AN INQUIRY INTO MEANING AND TRUTH ** \n",
    "The footer is all of the text after the line of text that says: THE END To have a better model, it is strongly recommended that you download the following books from The Library of Congress https://archive.org and convert them to text ﬁles:\n",
    "\n",
    "i. The History of Western Philosophy https://archive.org/details/westernphilosophy4\n",
    "\n",
    "ii. The Analysis of Matter https://archive.org/details/in.ernet.dli.2015.221533 \n",
    "\n",
    "iii. An Inquiry into Meaning and Truth https://archive.org/details/BertrandRussell-AnInquaryIntoMeaningAndTruth\n",
    "\n",
    "Try to only use the text of the books and throw away unwanted text before and after the text, although in a large corpus, these are considered as noise and should not make big problems.1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "vCqq2pzCGf8O"
   },
   "outputs": [],
   "source": [
    "file = open('TAM.txt', 'rt')\n",
    "text0  = file.read().lower()\n",
    "file.close()\n",
    "file = open('TPP.txt', 'rt')\n",
    "text1  = file.read().lower()\n",
    "file.close()\n",
    "file = open('OKEWFSMP.txt', 'rt')\n",
    "text2  = file.read().lower()\n",
    "file.close()\n",
    "file = open('MLOE.txt', 'rt')\n",
    "text4  = file.read().lower()\n",
    "file.close()\n",
    "file = open('THWP.txt', 'rt',encoding = \"ISO-8859-1\")\n",
    "text3  = file.read().lower()\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "yFfBm6edQuaB"
   },
   "outputs": [],
   "source": [
    "text=text0+text1+text2+text4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1525407291231,
     "user": {
      "displayName": "Tanay Shankar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "101648044313053317537"
     },
     "user_tz": 420
    },
    "id": "zsUKi1KeGf8i",
    "outputId": "943adf3a-8140-457c-8e35-ec83acdc4b08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffmysticism and logic and other essays\\n\\n\\n\\n\\ni\\n\\nmysticism and logic\\n\\n\\nmetaphysics, or the attempt to co'"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text4[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "83-uMC_eGf9b"
   },
   "outputs": [],
   "source": [
    "text = re.sub('[^a-zA-Z\\.\\,]',' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "7HCtkeNGGgC8"
   },
   "outputs": [],
   "source": [
    "text=re.sub( '\\s+', ' ', text ).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "NuEW670qGgDg"
   },
   "outputs": [],
   "source": [
    "alpharray=list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Vurpx_6_GgEm"
   },
   "outputs": [],
   "source": [
    "asciiarray=[ord(c) for c in alpharray]\n",
    "scaledarray=[((c-32)/90) for c in asciiarray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "d4ySs-rOGgGO"
   },
   "outputs": [],
   "source": [
    "tochar=dict((c,chr(int((c*90)+32))) for i, c in enumerate(set(scaledarray)))\n",
    "toascii=dict((chr(int((c*90)+32)),c) for i, c in enumerate(set(scaledarray)))\n",
    "del alpharray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1525407297590,
     "user": {
      "displayName": "Tanay Shankar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "101648044313053317537"
     },
     "user_tz": 420
    },
    "id": "s9ueILLzGM2r",
    "outputId": "5e0d0120-1c82-4c9d-bd1d-f91f4eda15b2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0.0,\n",
       " ',': 0.13333333333333333,\n",
       " '.': 0.15555555555555556,\n",
       " 'a': 0.7222222222222222,\n",
       " 'b': 0.7333333333333333,\n",
       " 'c': 0.7444444444444445,\n",
       " 'd': 0.7555555555555555,\n",
       " 'e': 0.7666666666666667,\n",
       " 'f': 0.7777777777777778,\n",
       " 'g': 0.7888888888888889,\n",
       " 'h': 0.8,\n",
       " 'i': 0.8111111111111111,\n",
       " 'j': 0.8222222222222222,\n",
       " 'k': 0.8333333333333334,\n",
       " 'l': 0.8444444444444444,\n",
       " 'm': 0.8555555555555555,\n",
       " 'n': 0.8666666666666667,\n",
       " 'o': 0.8777777777777778,\n",
       " 'p': 0.8888888888888888,\n",
       " 'q': 0.9,\n",
       " 'r': 0.9111111111111111,\n",
       " 's': 0.9222222222222223,\n",
       " 't': 0.9333333333333333,\n",
       " 'u': 0.9444444444444444,\n",
       " 'v': 0.9555555555555556,\n",
       " 'w': 0.9666666666666667,\n",
       " 'x': 0.9777777777777777,\n",
       " 'y': 0.9888888888888889,\n",
       " 'z': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "TzAmQgcrGgTN"
   },
   "outputs": [],
   "source": [
    "toenc={}\n",
    "inc=0\n",
    "for c in sorted(toascii.values()):\n",
    "    toenc.update({c:inc})\n",
    "    inc+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JgaJHLXDGgUI"
   },
   "outputs": [],
   "source": [
    "decode={}\n",
    "inc=0\n",
    "for c in sorted(toascii.values()):\n",
    "    decode.update({inc:c})\n",
    "    inc+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "m2L7uh8kGgbW"
   },
   "outputs": [],
   "source": [
    "chars=set(scaledarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JryZW5NTGgkA"
   },
   "source": [
    "\n",
    "iii. Choose a window size, e.g., W = 100.\n",
    "\n",
    "iv. Inputs to the network will be the ﬁrst W−1 = 99 characters of each sequence, and the output of the network will be the Lth character of the sequence. Basically, we are training the network to predict the each character using the 99 characters that precede it. Slide the window in strides of S = 1 on the text. For example, if W = 5 and S = 1 and we want to train the network with the sequence ABRACADABRA, The ﬁrst input to the network will be ABRA and the corresponding output will be C. The second input will be BRAC and the second output will be A, etc. \n",
    "\n",
    "v. Note that the output has to be encoded using a one-hot encoding scheme with N = 256 (or less) elements. This means that the network reads integers, but outputs a vector of N = 256 (or less) elements. \n",
    "\n",
    "vi. Use a single hidden layer for the LSTM with N = 256 (or less) memory units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9469,
     "status": "ok",
     "timestamp": 1525407310555,
     "user": {
      "displayName": "Tanay Shankar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "101648044313053317537"
     },
     "user_tz": 420
    },
    "id": "GG5L27ODGgb9",
    "outputId": "3d399c43-a407-4199-f7d4-36cbef6ba038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 1558506\n"
     ]
    }
   ],
   "source": [
    "N = 99\n",
    "\n",
    "def nSentences(fullarray,maxlen):\n",
    "    step = 1\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(fullarray) - maxlen, step):\n",
    "        sentences.append(fullarray[i: i + maxlen])\n",
    "        next_chars.append(fullarray[i + maxlen])\n",
    "    print('nb sequences:', len(sentences))\n",
    "    return sentences,next_chars\n",
    "sentences,next_char=nSentences(scaledarray,N)\n",
    "del scaledarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 110101,
     "status": "ok",
     "timestamp": 1525407420673,
     "user": {
      "displayName": "Tanay Shankar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "101648044313053317537"
     },
     "user_tz": 420
    },
    "id": "C1k_1b0CGgcq",
    "outputId": "29956635-cf4e-4a62-db38-c8afb7129d7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produce X and Y to train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1558506, 99, 29)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Produce X and Y to train')\n",
    "x = np.zeros((len(sentences), N, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "     for k , char in enumerate(sentence):\n",
    "        x[i, k , toenc[char]] = 1\n",
    "        y[i, toenc[next_char[i]]] = 1\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qDDqpACfeATC"
   },
   "outputs": [],
   "source": [
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1525407422447,
     "user": {
      "displayName": "Tanay Shankar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "101648044313053317537"
     },
     "user_tz": 420
    },
    "id": "HvPuq42VGghy",
    "outputId": "c8636d4b-64b8-4894-b595-4e89b7ec1dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               292864    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 29)                7453      \n",
      "=================================================================\n",
      "Total params: 300,317\n",
      "Trainable params: 300,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(N, len(chars))))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(len(chars),activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "x. Use model checkpointing to keep the network weights to determine each time an improvement in loss is observed at the end of the epoch. Find the best set of weights in terms of loss. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "vm66SbaIGgis"
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iBDOlgmMGgkF"
   },
   "source": [
    "vii. Use a Softmax output layer to yield a probability prediction for each of the characters between 0 and 1. This is actually a character classiﬁcation problem with N classes. Choose log loss (cross entropy) as the objective function for the network (research what it means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IQsXR0WAGgiw"
   },
   "outputs": [],
   "source": [
    "#optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4TJxMTUpGgkH"
   },
   "source": [
    "**Cross Entropy**\n",
    "\n",
    "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\n",
    "\n",
    "Given the range of possible loss values given a true observation. As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predications that are confident and wrong!\n",
    "\n",
    "Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing.\n",
    "\n",
    "\n",
    "In binary classification, where the number of classes M equals 2, cross-entropy can be calculated as:\n",
    "\n",
    "−(ylog(p)+(1−y)log(1−p))\n",
    "If M>2 (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqIObX5HGgi0"
   },
   "source": [
    "### (c) LSTM: Train an LSTM to mimic Russell’s style and thoughts:\n",
    "\n",
    "i. Concatenate text ﬁles to create a corpus of Russell’s writings. \n",
    "\n",
    "ii. Use a character-level representation for this model by using extended ASCII that has N = 256 characters. Each character will be encoded into a an integer using its ASCII code. Rescale the integers to the range [0,1], because LSTM uses a sigmoid activation function. LSTM will receive the rescaled integers as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 3590
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22268361,
     "status": "ok",
     "timestamp": 1525429691740,
     "user": {
      "displayName": "Tanay Shankar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "101648044313053317537"
     },
     "user_tz": 420
    },
    "id": "xMxEsmKqGgi1",
    "outputId": "ecf2a620-ff1b-46fc-dbcf-f86f3fcbafd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1558506/1558506 [==============================] - 446s 286us/step - loss: 2.5040\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.50396, saving model to weights-improvement-01-2.5040.hdf5\n",
      "Epoch 2/50\n",
      "1305000/1558506 [========================>.....] - ETA: 1:12 - loss: 1.98831558506/1558506 [==============================] - 444s 285us/step - loss: 1.9688\n",
      "\n",
      "Epoch 00002: loss improved from 2.50396 to 1.96880, saving model to weights-improvement-02-1.9688.hdf5\n",
      "Epoch 3/50\n",
      "1558506/1558506 [==============================] - 443s 284us/step - loss: 1.7560\n",
      "\n",
      "Epoch 00003: loss improved from 1.96880 to 1.75601, saving model to weights-improvement-03-1.7560.hdf5\n",
      "Epoch 4/50\n",
      " 110000/1558506 [=>............................] - ETA: 6:52 - loss: 1.67111558506/1558506 [==============================] - 449s 288us/step - loss: 1.6114\n",
      "\n",
      "Epoch 00004: loss improved from 1.75601 to 1.61138, saving model to weights-improvement-04-1.6114.hdf5\n",
      "Epoch 5/50\n",
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.51581558506/1558506 [==============================] - 445s 286us/step - loss: 1.5129\n",
      "\n",
      "Epoch 00005: loss improved from 1.61138 to 1.51292, saving model to weights-improvement-05-1.5129.hdf5\n",
      "Epoch 6/50\n",
      "1558506/1558506 [==============================] - 445s 285us/step - loss: 1.4412\n",
      "\n",
      "Epoch 00006: loss improved from 1.51292 to 1.44123, saving model to weights-improvement-06-1.4412.hdf5\n",
      "Epoch 7/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:48 - loss: 1.40221558506/1558506 [==============================] - 450s 289us/step - loss: 1.3876\n",
      "\n",
      "Epoch 00007: loss improved from 1.44123 to 1.38760, saving model to weights-improvement-07-1.3876.hdf5\n",
      "Epoch 8/50\n",
      "1435000/1558506 [==========================>...] - ETA: 35s - loss: 1.34601558506/1558506 [==============================] - 444s 285us/step - loss: 1.3449\n",
      "\n",
      "Epoch 00008: loss improved from 1.38760 to 1.34485, saving model to weights-improvement-08-1.3449.hdf5\n",
      "Epoch 9/50\n",
      "1558506/1558506 [==============================] - 445s 286us/step - loss: 1.3108\n",
      "\n",
      "Epoch 00009: loss improved from 1.34485 to 1.31080, saving model to weights-improvement-09-1.3108.hdf5\n",
      "Epoch 10/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:57 - loss: 1.29101558506/1558506 [==============================] - 450s 289us/step - loss: 1.2819\n",
      "\n",
      "Epoch 00010: loss improved from 1.31080 to 1.28189, saving model to weights-improvement-10-1.2819.hdf5\n",
      "Epoch 11/50\n",
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.25841558506/1558506 [==============================] - 446s 286us/step - loss: 1.2576\n",
      "\n",
      "Epoch 00011: loss improved from 1.28189 to 1.25759, saving model to weights-improvement-11-1.2576.hdf5\n",
      "Epoch 12/50\n",
      "1558506/1558506 [==============================] - 445s 286us/step - loss: 1.2377\n",
      "\n",
      "Epoch 00012: loss improved from 1.25759 to 1.23768, saving model to weights-improvement-12-1.2377.hdf5\n",
      "Epoch 13/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:48 - loss: 1.21881558506/1558506 [==============================] - 448s 288us/step - loss: 1.2198\n",
      "\n",
      "Epoch 00013: loss improved from 1.23768 to 1.21979, saving model to weights-improvement-13-1.2198.hdf5\n",
      "Epoch 14/50\n",
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.20411558506/1558506 [==============================] - 447s 287us/step - loss: 1.2038\n",
      "\n",
      "Epoch 00014: loss improved from 1.21979 to 1.20382, saving model to weights-improvement-14-1.2038.hdf5\n",
      "Epoch 15/50\n",
      "1558506/1558506 [==============================] - 443s 284us/step - loss: 1.1902\n",
      "\n",
      "Epoch 00015: loss improved from 1.20382 to 1.19024, saving model to weights-improvement-15-1.1902.hdf5\n",
      "Epoch 16/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:47 - loss: 1.17181558506/1558506 [==============================] - 444s 285us/step - loss: 1.1781\n",
      "\n",
      "Epoch 00016: loss improved from 1.19024 to 1.17811, saving model to weights-improvement-16-1.1781.hdf5\n",
      "Epoch 17/50\n",
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.16691558506/1558506 [==============================] - 444s 285us/step - loss: 1.1665\n",
      "\n",
      "Epoch 00017: loss improved from 1.17811 to 1.16651, saving model to weights-improvement-17-1.1665.hdf5\n",
      "Epoch 18/50\n",
      "1558506/1558506 [==============================] - 443s 284us/step - loss: 1.1561\n",
      "\n",
      "Epoch 00018: loss improved from 1.16651 to 1.15612, saving model to weights-improvement-18-1.1561.hdf5\n",
      "Epoch 19/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:47 - loss: 1.14631558506/1558506 [==============================] - 444s 285us/step - loss: 1.1475\n",
      "\n",
      "Epoch 00019: loss improved from 1.15612 to 1.14750, saving model to weights-improvement-19-1.1475.hdf5\n",
      "Epoch 20/50\n",
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.13831558506/1558506 [==============================] - 445s 285us/step - loss: 1.1381\n",
      "\n",
      "Epoch 00020: loss improved from 1.14750 to 1.13806, saving model to weights-improvement-20-1.1381.hdf5\n",
      "Epoch 21/50\n",
      "1558506/1558506 [==============================] - 445s 286us/step - loss: 1.1307\n",
      "\n",
      "Epoch 00021: loss improved from 1.13806 to 1.13075, saving model to weights-improvement-21-1.1307.hdf5\n",
      "Epoch 22/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:48 - loss: 1.12181558506/1558506 [==============================] - 445s 285us/step - loss: 1.1232\n",
      "\n",
      "Epoch 00022: loss improved from 1.13075 to 1.12319, saving model to weights-improvement-22-1.1232.hdf5\n",
      "Epoch 23/50\n",
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.11651558506/1558506 [==============================] - 445s 286us/step - loss: 1.1168\n",
      "\n",
      "Epoch 00023: loss improved from 1.12319 to 1.11681, saving model to weights-improvement-23-1.1168.hdf5\n",
      "Epoch 24/50\n",
      "1558506/1558506 [==============================] - 445s 286us/step - loss: 1.1100\n",
      "\n",
      "Epoch 00024: loss improved from 1.11681 to 1.10997, saving model to weights-improvement-24-1.1100.hdf5\n",
      "Epoch 25/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:47 - loss: 1.09651558506/1558506 [==============================] - 445s 285us/step - loss: 1.1035\n",
      "\n",
      "Epoch 00025: loss improved from 1.10997 to 1.10351, saving model to weights-improvement-25-1.1035.hdf5\n",
      "Epoch 26/50\n",
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.09851558506/1558506 [==============================] - 445s 285us/step - loss: 1.0989\n",
      "\n",
      "Epoch 00026: loss improved from 1.10351 to 1.09891, saving model to weights-improvement-26-1.0989.hdf5\n",
      "Epoch 27/50\n",
      "1558506/1558506 [==============================] - 445s 285us/step - loss: 1.0932\n",
      "\n",
      "Epoch 00027: loss improved from 1.09891 to 1.09319, saving model to weights-improvement-27-1.0932.hdf5\n",
      "Epoch 28/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:47 - loss: 1.07691558506/1558506 [==============================] - 445s 285us/step - loss: 1.0880\n",
      "\n",
      "Epoch 00028: loss improved from 1.09319 to 1.08795, saving model to weights-improvement-28-1.0880.hdf5\n",
      "Epoch 29/50\n",
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.08451558506/1558506 [==============================] - 445s 285us/step - loss: 1.0842\n",
      "\n",
      "Epoch 00029: loss improved from 1.08795 to 1.08424, saving model to weights-improvement-29-1.0842.hdf5\n",
      "Epoch 30/50\n",
      "1558506/1558506 [==============================] - 445s 286us/step - loss: 1.0791\n",
      "\n",
      "Epoch 00030: loss improved from 1.08424 to 1.07907, saving model to weights-improvement-30-1.0791.hdf5\n",
      "Epoch 31/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:48 - loss: 1.06801558506/1558506 [==============================] - 445s 286us/step - loss: 1.0749\n",
      "\n",
      "Epoch 00031: loss improved from 1.07907 to 1.07490, saving model to weights-improvement-31-1.0749.hdf5\n",
      "Epoch 32/50\n",
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.07111558506/1558506 [==============================] - 445s 286us/step - loss: 1.0714\n",
      "\n",
      "Epoch 00032: loss improved from 1.07490 to 1.07137, saving model to weights-improvement-32-1.0714.hdf5\n",
      "Epoch 33/50\n",
      "1558506/1558506 [==============================] - 444s 285us/step - loss: 1.0669\n",
      "\n",
      "Epoch 00033: loss improved from 1.07137 to 1.06692, saving model to weights-improvement-33-1.0669.hdf5\n",
      "Epoch 34/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:47 - loss: 1.05941558506/1558506 [==============================] - 445s 285us/step - loss: 1.0635\n",
      "\n",
      "Epoch 00034: loss improved from 1.06692 to 1.06346, saving model to weights-improvement-34-1.0635.hdf5\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.05931558506/1558506 [==============================] - 445s 285us/step - loss: 1.0598\n",
      "\n",
      "Epoch 00035: loss improved from 1.06346 to 1.05985, saving model to weights-improvement-35-1.0598.hdf5\n",
      "Epoch 36/50\n",
      "1558506/1558506 [==============================] - 445s 285us/step - loss: 1.0573\n",
      "\n",
      "Epoch 00036: loss improved from 1.05985 to 1.05727, saving model to weights-improvement-36-1.0573.hdf5\n",
      "Epoch 37/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:47 - loss: 1.05071558506/1558506 [==============================] - 445s 286us/step - loss: 1.0543\n",
      "\n",
      "Epoch 00037: loss improved from 1.05727 to 1.05432, saving model to weights-improvement-37-1.0543.hdf5\n",
      "Epoch 38/50\n",
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.05061558506/1558506 [==============================] - 445s 285us/step - loss: 1.0505\n",
      "\n",
      "Epoch 00038: loss improved from 1.05432 to 1.05045, saving model to weights-improvement-38-1.0505.hdf5\n",
      "Epoch 39/50\n",
      "1558506/1558506 [==============================] - 445s 285us/step - loss: 1.0472\n",
      "\n",
      "Epoch 00039: loss improved from 1.05045 to 1.04720, saving model to weights-improvement-39-1.0472.hdf5\n",
      "Epoch 40/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:47 - loss: 1.03841558506/1558506 [==============================] - 445s 285us/step - loss: 1.0451\n",
      "\n",
      "Epoch 00040: loss improved from 1.04720 to 1.04506, saving model to weights-improvement-40-1.0451.hdf5\n",
      "Epoch 41/50\n",
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.04171558506/1558506 [==============================] - 445s 285us/step - loss: 1.0423\n",
      "\n",
      "Epoch 00041: loss improved from 1.04506 to 1.04230, saving model to weights-improvement-41-1.0423.hdf5\n",
      "Epoch 42/50\n",
      "1558506/1558506 [==============================] - 444s 285us/step - loss: 1.0398\n",
      "\n",
      "Epoch 00042: loss improved from 1.04230 to 1.03977, saving model to weights-improvement-42-1.0398.hdf5\n",
      "Epoch 43/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:47 - loss: 1.03051558506/1558506 [==============================] - 445s 286us/step - loss: 1.0372\n",
      "\n",
      "Epoch 00043: loss improved from 1.03977 to 1.03723, saving model to weights-improvement-43-1.0372.hdf5\n",
      "Epoch 44/50\n",
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.03441558506/1558506 [==============================] - 446s 286us/step - loss: 1.0345\n",
      "\n",
      "Epoch 00044: loss improved from 1.03723 to 1.03455, saving model to weights-improvement-44-1.0345.hdf5\n",
      "Epoch 45/50\n",
      "1558506/1558506 [==============================] - 446s 286us/step - loss: 1.0322\n",
      "\n",
      "Epoch 00045: loss improved from 1.03455 to 1.03225, saving model to weights-improvement-45-1.0322.hdf5\n",
      "Epoch 46/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:49 - loss: 1.02341558506/1558506 [==============================] - 446s 286us/step - loss: 1.0302\n",
      "\n",
      "Epoch 00046: loss improved from 1.03225 to 1.03017, saving model to weights-improvement-46-1.0302.hdf5\n",
      "Epoch 47/50\n",
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.02861558506/1558506 [==============================] - 446s 286us/step - loss: 1.0284\n",
      "\n",
      "Epoch 00047: loss improved from 1.03017 to 1.02836, saving model to weights-improvement-47-1.0284.hdf5\n",
      "Epoch 48/50\n",
      "1558506/1558506 [==============================] - 446s 286us/step - loss: 1.0259\n",
      "\n",
      "Epoch 00048: loss improved from 1.02836 to 1.02587, saving model to weights-improvement-48-1.0259.hdf5\n",
      "Epoch 49/50\n",
      " 130000/1558506 [=>............................] - ETA: 6:49 - loss: 1.01621558506/1558506 [==============================] - 446s 286us/step - loss: 1.0241\n",
      "\n",
      "Epoch 00049: loss improved from 1.02587 to 1.02413, saving model to weights-improvement-49-1.0241.hdf5\n",
      "Epoch 50/50\n",
      "1430000/1558506 [==========================>...] - ETA: 36s - loss: 1.02201558506/1558506 [==============================] - 446s 286us/step - loss: 1.0223\n",
      "\n",
      "Epoch 00050: loss improved from 1.02413 to 1.02229, saving model to weights-improvement-50-1.0223.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fddaa1e9390>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y, epochs=50, batch_size=5000,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1525431751878,
     "user": {
      "displayName": "Tanay Shankar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "101648044313053317537"
     },
     "user_tz": 420
    },
    "id": "4htmaxUwGgjJ",
    "outputId": "9076ff11-7dce-4a1b-e253-c347d6c59977"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques=\"There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.\"\n",
    "ques=ques.lower()\n",
    "ques=ques[-N:]\n",
    "len(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "F4XP4qXwGgjS"
   },
   "outputs": [],
   "source": [
    "testasciiarray=[ord(c) for c in ques]\n",
    "testscaledarray=[((c-32)/90) for c in testasciiarray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Ndy9gIHiGgjc"
   },
   "outputs": [],
   "source": [
    "testsentences=[testscaledarray]\n",
    "x_t = np.zeros((len(testsentences), N, len(chars)), dtype=np.bool)\n",
    "for i, tsentence in enumerate(testsentences):\n",
    "     for k , char in enumerate(tsentence):\n",
    "        x_t[i, k , toenc[char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "x-_BQ0elGgjk"
   },
   "outputs": [],
   "source": [
    "x_t.shape\n",
    "prediction = model.predict(x_t, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9Orbv5eGgkI"
   },
   "source": [
    "viii. We do not use a test dataset. We are using the whole training dataset to learn the probability of each character in a sequence. We are not seeking for a very accurate model of. Instead we are interested in a generalization of the dataset that can mimic the gist of the text. \n",
    "\n",
    "ix. Choose a reasonable number of epochs for training (e.g., 30, although the network will need more epochs to yield a better model). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result after a initial few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 69453,
     "status": "ok",
     "timestamp": 1525432358492,
     "user": {
      "displayName": "Tanay Shankar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "101648044313053317537"
     },
     "user_tz": 420
    },
    "id": "7tzXFg5G1454",
    "outputId": "1b28bcba-5fdd-4a44-a044-54398568cdb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t as they would physical phenomena. this school of psychologists tends not to emphasize the object.\n",
      "Generated text :\n",
      "ing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the same thing in the"
     ]
    }
   ],
   "source": [
    "print(ques)\n",
    "print(\"Generated text :\")\n",
    "for i in range(1000):\n",
    "    prediction = model.predict(x_t, verbose=0)\n",
    "    print(tochar[decode[np.argmax(prediction)]],end=\"\")\n",
    "    x_t[0][:-1]=x_t[0][1:]\n",
    "    k=np.zeros(len(chars), dtype=np.bool)\n",
    "    k[np.argmax(prediction)]=1\n",
    "    x_t[0][-1]=k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hwJ8xjNUGgkJ"
   },
   "source": [
    "xi. Use the network with the best weights to generate 1000 characters, using the following text as initialization of the network: There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement after several epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 70564,
     "status": "ok",
     "timestamp": 1525431867764,
     "user": {
      "displayName": "Tanay Shankar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "101648044313053317537"
     },
     "user_tz": 420
    },
    "id": "dxUQL0iYGgj1",
    "outputId": "35890e7c-f483-46d0-c4af-19133538d5e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t as they would physical phenomena. this school of psychologists tends not to emphasize the object.\n",
      "Generated text :\n",
      "s and the sense data which are the same thing to the problem of the proposition that the sense data is a proposition as the problem of the proposition is that the same thing is the same thing as the true interests of the sense data and the same thing as the subject of the sense data and the same thing as the subjective content of the proposition in the same particular theory of propositions and the sense data which are the same thing to the problem of the proposition that the sense data is a proposition as the problem of the proposition is that the same thing is the same thing as the true interests of the sense data and the same thing as the subject of the sense data and the same thing as the subjective content of the proposition in the same particular theory of propositions and the sense data which are the same thing to the problem of the proposition that the sense data is a proposition as the problem of the proposition is that the same thing is the same thing as the true interests of"
     ]
    }
   ],
   "source": [
    "print(ques)\n",
    "print(\"Generated text :\")\n",
    "for i in range(1000):\n",
    "    prediction = model.predict(x_t, verbose=0)\n",
    "    print(tochar[decode[np.argmax(prediction)]],end=\"\")\n",
    "    x_t[0][:-1]=x_t[0][1:]\n",
    "    k=np.zeros(len(chars), dtype=np.bool)\n",
    "    k[np.argmax(prediction)]=1\n",
    "    x_t[0][-1]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27965,
     "status": "ok",
     "timestamp": 1525430481537,
     "user": {
      "displayName": "Tanay Shankar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "101648044313053317537"
     },
     "user_tz": 420
    },
    "id": "6WdDz-YZu13W",
    "outputId": "4396ceef-a138-493e-c07a-b32ace045649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t as they would physical phenomena. this school of psychologists tends not to emphasize the object.\n",
      "Generated text :\n",
      "m of the proposition is that the same thing is the same thing as the true interests of the sense data and the same thing as the subject of the sense data and the same thing as the subjective content of the proposition in the same particular theory of propositions and the sense data which are the same thing to the problem of the proposition that the sense data is a proposition as the problem of the"
     ]
    }
   ],
   "source": [
    "print(ques)\n",
    "print(\"Generated text :\")\n",
    "for i in range(400):\n",
    "    prediction = model.predict(x_t, verbose=0)\n",
    "    print(tochar[decode[np.argmax(prediction)]],end=\"\")\n",
    "    x_t[0][:-1]=x_t[0][1:]\n",
    "    k=np.zeros(len(chars), dtype=np.bool)\n",
    "    k[np.argmax(prediction)]=1\n",
    "    x_t[0][-1]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 70126,
     "status": "ok",
     "timestamp": 1525431224152,
     "user": {
      "displayName": "Tanay Shankar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "101648044313053317537"
     },
     "user_tz": 420
    },
    "id": "9ITsm2H-u2D2",
    "outputId": "8e8d5556-408b-4769-f095-48d002483713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t as they would physical phenomena. this school of psychologists tends not to emphasize the object.\n",
      "Generated text :\n",
      "ame thing is the same thing to be a property of the proposition that there is a constituent of the proposition in the same particular thing and the sense data of sense data, and therefore there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a constituent of the proposition that there is a co"
     ]
    }
   ],
   "source": [
    "print(ques)\n",
    "print(\"Generated text :\")\n",
    "for i in range(1000):\n",
    "    prediction = model.predict(x_t, verbose=0)\n",
    "    print(tochar[decode[np.argmax(prediction)]],end=\"\")\n",
    "    x_t[0][:-1]=x_t[0][1:]\n",
    "    k=np.zeros(len(chars), dtype=np.bool)\n",
    "    k[np.argmax(prediction)]=1\n",
    "    x_t[0][-1]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 69724,
     "status": "ok",
     "timestamp": 1525431751621,
     "user": {
      "displayName": "Tanay Shankar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "101648044313053317537"
     },
     "user_tz": 420
    },
    "id": "eSARl10bu2L6",
    "outputId": "c10f00d8-7913-4b3d-acbb-90067c1e8591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d a specially intimate relation. there was a military aristocracy, and also a priestly aristocracy.\n",
      "Generated text :\n",
      "ing is the same thing as the true interests of the sense data and the same thing as the subject of the sense data and the same thing as the subjective content of the proposition in the same particular theory of propositions and the sense data which are the same thing to the problem of the proposition that the sense data is a proposition as the problem of the proposition is that the same thing is the same thing as the true interests of the sense data and the same thing as the subject of the sense data and the same thing as the subjective content of the proposition in the same particular theory of propositions and the sense data which are the same thing to the problem of the proposition that the sense data is a proposition as the problem of the proposition is that the same thing is the same thing as the true interests of the sense data and the same thing as the subject of the sense data and the same thing as the subjective content of the proposition in the same particular theory of propo"
     ]
    }
   ],
   "source": [
    "print(ques)\n",
    "print(\"Generated text :\")\n",
    "for i in range(1000):\n",
    "    prediction = model.predict(x_t, verbose=0)\n",
    "    print(tochar[decode[np.argmax(prediction)]],end=\"\")\n",
    "    x_t[0][:-1]=x_t[0][1:]\n",
    "    k=np.zeros(len(chars), dtype=np.bool)\n",
    "    k[np.argmax(prediction)]=1\n",
    "    x_t[0][-1]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "XgrJHEVAL4Om"
   },
   "outputs": [],
   "source": [
    "#Load backed up model\n",
    "filename = \"weights-improvement-03-1.7560.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "oBO8YRp_GgkJ"
   },
   "source": [
    "## Conclusion\n",
    "####  LSTM  repeats most occuring elements such as space and articles when trained over a few epochs.\n",
    "#### As the number of neurons are increased along with the epochs the model learns more words and their usage. Although with repetition of phrases later, finally I was able to generate slightly meaningful text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ZX5FcB0GgkN"
   },
   "source": [
    "Future Work:\n",
    "\n",
    "Use one-hot encoding for the input sequence. Use a large number of epochs, e.g., 150. Add dropout to the network, and use a deeper LSTM (e.g. with 3 or more layers). Generate 3000 characters using the above initialization and check if we get more meaningful text. \n",
    "        \n",
    "Train a Hidden Markov Model with V hidden states and V possible outputs using Baum-Welch Algorithm (or any other modern algorithm that is available) using the Russell corpus, where V is the number of distinct words in the corpus."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "Homework6.1.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
